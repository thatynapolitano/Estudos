{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9fa0ea-5c11-4ede-b195-e51f5dd39cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introdução ao Pyspark e Persistência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e714b1-b919-491d-a4b3-d923cf8ffc63",
     "showTitle": false,
     "title": ""
    },
    "id": "0iD1RDv7jDIY"
   },
   "source": [
    "## 1. Introdução Pyspark\n",
    "\n",
    "Pandas e PySpark são bibliotecas populares no ecossistema Python para trabalhar com dados estruturados. Eles oferecem recursos de manipulação e análise de dados, mas diferem significativamente em termos de escalabilidade, desempenho e recursos de computação distribuída. Aqui, exploraremos as principais diferenças entre Pandas e PySpark DataFrames, juntamente com exemplos de uso de ambos em Python.\n",
    "\n",
    "**1. Escalabilidade:**\n",
    "\n",
    "- **Pandas:** O Pandas opera na memória e é adequado para lidar com conjuntos de dados menores que cabem na RAM disponível. Ele é projetado principalmente para processamento em uma única máquina.\n",
    "   \n",
    "- **PySpark:** O PySpark, por outro lado, foi projetado para computação distribuída e pode lidar com grandes conjuntos de dados que excedem a capacidade de memória de uma única máquina. Ele distribui dados por um cluster de máquinas, permitindo o processamento paralelo.\n",
    "\n",
    "<img src=\"https://www.element61.be/sites/default/files/img_knowledge_base/PandasVsPySpark1.png\" width=380 text=\"https://www.element61.be/en/resource/how-use-python-and-pandas-while-embracing-power-spark\">\n",
    "\n",
    "<img src=\"https://www.element61.be/sites/default/files/img_knowledge_base/PandasVsPySpark2.png\" width=380 text=\"https://www.element61.be/en/resource/how-use-python-and-pandas-while-embracing-power-spark\">\n",
    "\n",
    "**2. Desempenho:**\n",
    "\n",
    "- **Pandas:** O Pandas é otimizado para processamento em uma única máquina, o que o torna muito eficiente para conjuntos de dados de pequeno e médio porte. No entanto, pode ter dificuldades com conjuntos de dados maiores devido a limitações de memória.\n",
    "   \n",
    "- **PySpark:** o desempenho do PySpark é dimensionado de acordo com o tamanho do cluster, tornando-o adequado para processamento de big data. Ele aproveita mecanismos de computação distribuída como Apache Spark para alcançar alto desempenho.\n",
    "\n",
    "<img src=\"https://liliasfaxi.github.io/Atelier-Spark/img/p4/rdd.png\" width=500 text=\"https://liliasfaxi.github.io/Atelier-Spark/p4-batch/\">\n",
    "\n",
    "\n",
    "**3. Fácil de usar:**\n",
    "\n",
    "- **Pandas:** O Pandas fornece uma API intuitiva e fácil de usar que é amplamente adotada por analistas de dados e cientistas. É conhecido por sua facilidade de uso e poderosos recursos de manipulação de dados.\n",
    "   \n",
    "- **PySpark:** a API Pandas do PySpark é semelhante ao Pandas, facilitando a transição dos usuários do Pandas. No entanto, configurar um cluster Spark e lidar com conceitos de computação distribuída pode ser mais complexo.\n",
    "\n",
    "**4. Criação de DataFrame:**\n",
    "\n",
    "- **Pandas:** Pandas DataFrames são criados a partir de várias fontes de dados, como arquivos CSV, planilhas Excel, bancos de dados SQL e dicionários Python.\n",
    "\n",
    "- **PySpark:** PySpark DataFrames normalmente são criados a partir de fontes de dados distribuídas, como HDFS, Apache Hive, Apache HBase ou RDDs (Resilient Distributed Datasets) existentes.\n",
    "\n",
    "**5. Casos de uso:**\n",
    "\n",
    "- **Pandas:** O Pandas é ideal para exploração, análise e pré-processamento de dados em conjuntos de dados menores, especialmente em um ambiente de máquina única. É uma ferramenta indispensável para muitos cientistas de dados.\n",
    "\n",
    "- **PySpark:** O PySpark foi projetado para processamento de big data e é mais adequado para situações em que os dados não cabem na memória ou quando você precisa aproveitar a computação distribuída para tarefas como ETL (Extrair, Transformar, Carregar) e tarefas de grande porte. análise de dados em escala.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eo9rDC-kvAxCtP1dE5iZdg.png\" width=600 text=\"https://medium.com/@rana.akansha321/an-introduction-to-pyspark-powering-big-data-processing-with-python-e398cfb6edd9\">\n",
    "\n",
    "Em resumo, Pandas e PySpark DataFrames atendem a propósitos diferentes no ecossistema de processamento de dados. O Pandas é excelente para trabalhar com conjuntos de dados de pequeno e médio porte em uma única máquina, enquanto o PySpark é excelente para lidar com tarefas de processamento de dados distribuídos em grande escala. A escolha entre eles depende do tamanho específico dos dados e dos requisitos de processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ffd066-68d2-4d91-a738-c7de24c52e78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exemplos Pyspark x Pandas\n",
    "\n",
    "Vamos nos aprofundar nas diferenças entre Pandas e PySpark DataFrames e mostrar várias operações usando ambas as bibliotecas, incluindo leitura de arquivos CSV e JSON, adição de colunas, filtragem de dados e remoção de duplicatas.\n",
    "\n",
    "**1. Lendo arquivos CSV:**\n",
    "\n",
    "\n",
    "- **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   pandas_df = pd.read_csv('data.csv')\n",
    "   ```\n",
    "\n",
    "- **PySpark:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb085cd8-d8e9-4250-a558-80c6b17d5aea",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: Row(_c0=0, id=178, escolas_postos='HENFIL', bairro='CAJU', endereço='RUA CARLOS SEIDL', lat=-22.88089, lon=-43.22533, quantidade=20, subprefeitura='CENTRO', tipo_escola='CIEP', numero='S/N')"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196d2446-c50e-4603-a539-f40d08bb3881",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- `SparkSession.builder`: Inicia a construção de um `SparkSession`. O `SparkSession` é uma interface unificada para todos os recursos do Spark, incluindo SQL, DataFrames, DataSets, MLlib e GraphX.\n",
    "\n",
    "- `.appName('Aula PySpark')`: Define o nome da aplicação que será exibido nos logs do Spark. Isso ajuda na identificação e rastreamento da aplicação em um ambiente de cluster.\n",
    "\n",
    "- `.getOrCreate()`: Cria um novo `SparkSession` com as configurações especificadas ou retorna um `SparkSession` existente com o mesmo nome, se já existir. Isso é útil para evitar a criação de múltiplos `SparkSession`s com o mesmo nome, o que poderia levar a conflitos de recursos e estado.\n",
    "\n",
    "Essa etapa só não é obrigatória dentro do Databricks, uma vez que ele foi criado para ser usado com spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4f7d65-0395-4f9b-a5f5-7ade8d55472c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nome_arquivo = '/FileStore/tables/df_escolas.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5beb195d-646e-4fe9-a4fa-c73398b6a03d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**2. Lendo JSON:**\n",
    "\n",
    "   - **Pandas:**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   pandas_df = pd.read_json('data.json')\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   \n",
    "   spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "\n",
    "   pyspark_df = spark.read.json('data.json')\n",
    "   ```\n",
    "\n",
    "**3. Adicionando colunas:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   \n",
    "   pandas_df['New_Column'] = 1\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import lit\n",
    "\n",
    "   pyspark_df = pyspark_df.withColumn('New_Column', lit(1))\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f33dab-336e-49d3-9a9f-0b7276feea8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78451db-04e9-4fb7-9b4d-3a01581eee45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**4. Filtrando os dados:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   filtered_df = pandas_df[pandas_df['Age'] > 25]\n",
    "   ```\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import col\n",
    "\n",
    "   filtered_df = pyspark_df.filter(col('Age') > 25)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0879b4d-2b83-4e57-8ca1-b8c88f1af43e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf7f1a4-1ec5-479d-9024-ebbbb1ed05b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**5. Verificando Não nulos:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   pandas_df = pandas_df[pandas_df.col1.notnull()]\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   \n",
    "   pyspark_df = pyspark_df.where(col(\"col1\").isNolNull())\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c816be9c-4dab-46e0-8154-d7032188a6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84090367-1506-42db-ad19-da3fd7477353",
     "showTitle": false,
     "title": ""
    },
    "id": "mjE4wsmtkMVn"
   },
   "source": [
    "**6. Excluindo duplicatas:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   pandas_df = pandas_df.drop_duplicates()\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   \n",
    "   pyspark_df = pyspark_df.dropDuplicates()\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f31748c5-2feb-4aec-9fc8-400a25a28c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc32bcab-c713-4e67-9625-3d87527a94b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**7. Seleciona colunas:**\n",
    "\n",
    "   - **Pandas:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   lista_colunas = [\"col1\", \"col2\"]\n",
    "   pandas_df = pandas_df[lista_colunas]\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "   - **PySpark:**\n",
    "\n",
    "   ```python\n",
    "   lista_colunas = [\"col1\", \"col2\"]\n",
    "   pyspark_df = pyspark_df.select(lista_colunas)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6cbe13a-2bf6-4b36-80b0-ae0ebe1858aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da95aaeb-4a30-42d3-acb9-dff9b61f426c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**8. Cria df:**\n",
    "\n",
    "- **Pandas:**\n",
    "\n",
    "```python\n",
    "data = [{\"col1\":\"2   \", \"col2\" : \"1\", \"col3\":\"3\"},\n",
    "        {\"col1\":\"2    \", \"col2\" : \"1\", \"col3\":\"3\"},\n",
    "        {\"col1\":\"2     \", \"col2\" : \"1\", \"col3\":\"3\"}]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "```\n",
    "<br>\n",
    "\n",
    "   - **PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f20daf2-0be7-4f23-8df5-b36d4175a5d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [{\"col1\":\"2   \", \"col2\" : \"1\", \"col3\":\"3\"},\n",
    "        {\"col1\":\"2    \", \"col2\" : \"1\", \"col3\":\"3\"},\n",
    "        {\"col1\":\"2     \", \"col2\" : \"1\", \"col3\":\"3\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea0fee13-3e65-452e-946f-7dbf45a90f60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**8. Converte df Pyspark <> Pandas:**\n",
    "\n",
    "- **Pyspark para Pandas:**\n",
    "\n",
    "```python\n",
    "df = pyspark_df.toPandas()\n",
    "```\n",
    "<br>\n",
    "\n",
    "   - **Pandas para PySpark:**\n",
    "\n",
    "```python\n",
    "df_spark = spark.createDataFrame(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa896f9-e646-434c-9287-9dfb3510196c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Persistência de Dados\n",
    "\n",
    "Persistir dados em vários formatos de arquivo como CSV, JSON, Parquet e outros é uma tarefa comum no processamento e análise de dados e no dia a dia de um Engenheiro de Dados. Python, Pandas e PySpark fornecem ferramentas e bibliotecas poderosas para fazer isso.\n",
    "\n",
    "**1. CSV (valores separados por vírgula):**\n",
    "\n",
    "**Exemplo - Escrevendo CSV Python:**\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "data = [\n",
    "    [\"Name\", \"Age\", \"City\"],\n",
    "    [\"Alice\", 25, \"New York\"],\n",
    "    [\"Bob\", 30, \"Los Angeles\"],\n",
    "    [\"Charlie\", 28, \"Chicago\"]\n",
    "]\n",
    "\n",
    "with open(\"/FileStore/tables/data.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "```\n",
    "**Exemplo - Escrevendo CSV com Pandas:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 28],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"/FileStore/tables/data.csv\", index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97fcf221-3771-4a59-b32d-644c637e8d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemplo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Dados\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 28],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "\n",
    "# Criar DataFrame a partir dos dados\n",
    "df = spark.createDataFrame(list(zip(data[\"Name\"], data[\"Age\"], data[\"City\"])), [\"Name\", \"Age\", \"City\"])\n",
    "\n",
    "# Salvar o DataFrame em formato CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d42a3e9-08d0-4a8a-90b1-066233826314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. JSON :**\n",
    "\n",
    "\n",
    "**Exemplo - Escrevendo JSON Python:**\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"Name\": \"Alice\",\n",
    "    \"Age\": 25,\n",
    "    \"City\": \"New York\"\n",
    "}\n",
    "\n",
    "with open(\"/FileStore/tables/data.json\", 'w') as file:\n",
    "    json.dump(data, file)\n",
    "```\n",
    "**Exemplo - Escrevendo JSON com Pandas:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 28],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_json(\"/FileStore/tables/data.json\", orient=\"records\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5720971b-dc1a-4560-a924-36068789ccbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c241c13-ce31-41e1-8a2b-48bfb0165fec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**3. Parquet:**\n",
    "\n",
    "Parquet é um formato de arquivo de armazenamento colunar altamente eficiente para processamento de big data. No PySpark, você pode usar o módulo `pyspark.sql` para ler e escrever arquivos Parquet.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:887/1*0yVtTZ6MSR-S_uF2YeeUqQ.png\" width=500>\n",
    "\n",
    "**Exemplo - Escrevendo Parquet com Pandas:**\n",
    "```python\n",
    "# Salvar o DataFrame em formato Parquet\n",
    "df.to_parquet(\"/FileStore/tables/data.parquet\", index=False)\n",
    "```\n",
    "\n",
    "**Exemplo - Escrevendo Parquet com PySpark:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1193c8c-0126-4508-8091-a7844a5c1473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46a6b334-01fa-4456-b718-1d8542f1bc46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parquet x CSV\n",
    "\n",
    "Podemos comparar o uso do Parquet e o uso do CSV da seguinte forma:\n",
    "\n",
    "- Armazenamento de dados: O Parquet é um formato de armazenamento de dados de colunas otimizado para o processamento de grandes quantidades de dados em sistemas distribuídos, enquanto o CSV é um formato de arquivo simples e amplamente utilizado para armazenar dados em tabelas, com suporte a tipos de dados simples, como inteiros, reais e strings;\n",
    "\n",
    "- Eficiência de armazenamento: O Parquet oferece compressão eficiente e armazenamento de dados estruturados de forma eficiente, enquanto o CSV não oferece compressão e armazenamento de dados estruturados pode ser ineficiente para grandes quantidades de dados;\n",
    "\n",
    "- Leitura e escrita: O Parquet é otimizado para realizar leituras rápidas de subconjuntos de colunas, enquanto o CSV é mais adequado para a leitura e escrita de dados em uma tabela simples; e\n",
    "    Integração com ferramentas: O Parquet é suportado por diversas ferramentas de análise de dados, enquanto o CSV é compatível com a maioria das ferramentas de análise de dados e de planilhas.\n",
    "\n",
    "Em resumo, o Parquet é uma boa opção para o armazenamento de grandes quantidades de dados em sistemas distribuídos, enquanto o CSV é uma opção mais adequada para o armazenamento de dados simples em tabelas em uma única máquina.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1266864e-f447-4d48-9a0a-50b3f03fed98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**4. Outros formatos de arquivo:**\n",
    "\n",
    "Existem muitos outros formatos de arquivo comumente usados para persistência de dados, como Avro, ORC e Feather. O PySpark também suporta muitos desses formatos e você pode usá-los de forma semelhante à forma como demonstramos o Parquet.\n",
    "\n",
    "**Exemplo - Escrevendo Avro com PySpark:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AvroExample\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25, \"New York\"), (\"Bob\", 30, \"Los Angeles\"), (\"Charlie\", 28, \"Chicago\")]\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.format(\"avro\").save(\"data.avro\")\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "<center>\n",
    "<img src=\"https://datacrump.com/wp-content/uploads/2023/04/format_cover.png\" text=\"https://datacrump.com/csv-parquet-json-avro/\" width=600>\n",
    "</center>\n",
    "\n",
    "Em resumo, Python, Pandas e PySpark oferecem maneiras versáteis de persistir dados em vários formatos de arquivo, facilitando o trabalho com diferentes fontes e ferramentas de dados em seus pipelines de processamento e análise de dados. Você pode escolher o formato que melhor atende às suas necessidades com base em fatores como eficiência, legibilidade e compatibilidade com outros sistemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c65aa07-0896-4c46-bf7f-6710ea4567b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bibliografia e material de aprofundamento\n",
    "<br>\n",
    "\n",
    "- https://medium.com/@rana.akansha321/an-introduction-to-pyspark-powering-big-data-processing-with-python-e398cfb6edd9\n",
    "- https://datacrump.com/csv-parquet-json-avro/\n",
    "- https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705\n",
    "\n",
    "\n",
    "## [Avaliação anônima](https://forms.gle/tShxhxNYhvi6ZmQm8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728d8e8e-b85c-4ec1-a9cf-2c4244a4d0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercício\n",
    "\n",
    "Converta o código da aula de exercício para pyspark, rode para 100 amostras ou mais, salve o arquivo final em csv e parquet e compare o tamanho dos arquivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d8d2d58-3732-4b56-9b85-1ab0c3531372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "8 - Introdução ao Pyspark e Persistência dos Dados",
   "widgets": {}
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
